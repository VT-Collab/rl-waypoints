<!DOCTYPE html>
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

<meta charset="utf-8">
<meta name="description" content="Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks">
<meta name="keywords" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

  <link rel="icon" href="./favicon.ico?">
  
  <meta property="og:site_name" content="Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks" />
  <meta property="og:url" content="https://collab.me.vt.edu/rl-waypoints/" />
  <meta property="og:image" content="./static/resources/front.png" />
  <meta property="og:image:secure" content="./static/resources/front.png" />
  <meta property="og:video" content="https://youtu.be/ZC3BjY1k18w" />
  <meta property="og:video:secure" content="https://youtu.be/ZC3BjY1k18w" />



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://mehtashaunak.github.io">Shaunak A. Mehta</a>,</span>
            <span class="author-block"><a href="https://soheilhbn.com/">Soheil Habibian</a>,</span>
            <span class="author-block"><a href="https://dylanlosey.com/">Dylan P. Losey</a>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Collab, Virginia Tech</span>
            <br>
            <span class="brmod">Accepted at IEEE International Conference on Intelligent Robot Systems (IROS) 2024</span>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/resources/rl_waypoints.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.13281"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MMEd-lYfq4Y"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/VT-Collab/rl-waypoints"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="content">
            <h2 class="subtitle has-text-centered">
              <strong>Robot opening a drawer with a policy trained using our approach</strong>
            </h2>
            <video autoplay muted loop width="360">
              <source src="./static/resources/intro.mp4" type="video/mp4">
            </video>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <!-- Abstract. -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Robot arms should be able to learn new tasks.
              One framework here is reinforcement learning, where the robot is given a reward function that encodes the task, and the robot autonomously learns actions to maximize its reward.
              Existing approaches to reinforcement learning often frame this problem as a Markov decision process, and learn a policy (or a hierarchy of policies) to complete the task.
              These policies reason over hundreds of fine-grained actions that the robot arm needs to take: e.g., moving slightly to the right or rotating the end-effector a few degrees.
              But the manipulation tasks that we want robots to perform can often be broken down into a small number of high-level motions: e.g., reaching an object or turning a handle.
              In this paper we therefore propose a waypoint-based approach for model-free reinforcement learning.
              Instead of learning a low-level policy, the robot now learns a trajectory of waypoints, and then interpolates between those waypoints using existing controllers.
              Our key novelty is framing this waypoint-based setting as a sequence of multi-armed bandits: each bandit problem corresponds to one waypoint along the robot's motion.
              We theoretically show that an ideal solution to this reformulation has lower regret bounds than standard frameworks.
              We also introduce an approximate posterior sampling solution that builds the robot's motion one waypoint at a time.
              Results across benchmark simulations and two real-world experiments suggest that this proposed approach learns new tasks more quickly than state-of-the-art baselines. 
            </p>
          </div>
        </div>
      </div>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-max-desktop">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MMEd-lYfq4Y"
                  frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    </div>
    <!--/ Abstract. -->

    

    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="columns is-centered">
    <div class="column is-half">
      <h2 class="title is-2" style="text-align: center;">How Does it Work?</h2>
      <tr>
        <td>
          <br>
          <div class="columns is-centered has-text-centered">
            <img src="./static/resources/front.png" style="width: 80%;"></img>
          </div>
        </td>
      </tr>
      <div class="is-vcentered interpolation-panel">
        <div class="content has-text-justified">
          <p style = "font-size: 18px">
            We frame model-free reinforcement learning as a continuous multi-arm bandit problem. Each bandit problem corresponds to one waypoint along the robot's learned trajectory. 
            The lower bounds on the regret for this multi-arm bandit setting derived in the manuscript show that waypoint based reinforcement learning is linear in time complexity, i.e., the time needed to train for a given task increases linearly with the increase in the number of waypoints needed to solve the task.
            <br>
            The robot arm learns to place the next waypoint to maximize its reward by solving a multi-arm bandit problem. Once the robot has learned to correctly place a waypoint i, we freeze the learned models for that waypoint. To learn the next waypoint, the robot rolls out the trajectory for waypoints 1 to i using the saved models and then repeats the learning process for waypoint i+1. 
          </p>
        </div>
      </div>
  </div>
  </div>


</section>

<section class="hero teaser">
  <div class="hero-body">
    <!-- Simulation 1. -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-2">Benchmark Simulations</h2>
          <div class="content has-text-justified">
            <p>
              We evaluate the performance of our proposed approach in Robosuite, a simulated robot environment with a set of standard manipulation tasks for robot arms. Across these benchmark manipulation tasks, we compare the performance of out approach to that of standard reinforcement learning algorithms SAC and PPO as well as extended versions of these standard algorithms SAC-wp and PPO-wp. Below we show a video comparing the performance of Ours to that of SAC (the best performning baseline). The detailed resluts and comparisons with other baselines are discussed in the manuscript.
            </p>
          </div>
          <video autoplay muted loop width="1080">
            <source src="./static/resources/sims.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Simulation 1. -->
</section>


<section class="hero teaser">
  <div class="hero-body">
    <!-- Air Hockey. -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-2">Real-World Experiments</h2>
          <div class="content has-text-justified">
            <p>
              To evaluate if our proposed algorihtm can be applied to real world settings, i.e. trainig from scratch in the real world, we train a 7 DoF Franka Emika robot arm to perform two tasks in the real world. The first task is similar to the task in the simulation environment where the robot needs to pick up a block placed on a table. In the sencond task, the robot needs to open a drawer placed in its workspace. In this real world setting, we compare the performance of our proposed approach to that of the best performing baseline from the simulation SAC. The videos for the performance of the robot across both the tasks are shown below.
            </p>
            <h2 class="subtitle has-text-centered">
              <strong>Picking-up a Block</strong>
            </h2>
            <video autoplay muted loop width="1080">
              <source src="./static/resources/pick_robot.mp4" type="video/mp4">
            </video>

            <h2 class="subtitle has-text-centered">
              <strong>Opening a Drawer</strong>
            </h2>
            <video autoplay muted loop width="1080">
              <source src="./static/resources/drawer_robot.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>

    
    <!--/ Air Hockey. -->
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{mehta2024waypoint,
      title={Waypoint-Based Reinforcement Learning for Robot Manipulation Tasks},
      author={Mehta, Shaunak A and Habibian, Soheil and Losey, Dylan P},
      journal={arXiv preprint arXiv:2403.13281},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
      href="">
     <i class="fas fa-file-pdf"></i>
   </a>
   <a class="icon-link" href="https://github.com/VT-Collab/Stable-BC" class="external-link" disabled>
     <i class="fab fa-github"></i>
   </a>
      <p>Page template borrowed from  <a href="https://human2robot.github.io/"><span class="dnerf">Whirl</span></a>, <a href="https://nerfies.github.io"><span class="dnerf">Nerfies</span></a>, <a href="https://energy-locomotion.github.io/"><span class="dnerf">Energy Locomotion</span></a> and <a href="https://robotic-telekinesis.github.io/"><span class="dnerf">Robotic Telekinesis</span></a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
